{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain chromadb faiss-cpu openai tiktoken langchain_openai langchain-community wikipedia"
      ],
      "metadata": {
        "id": "TQVEdlAPEQI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-chroma -q"
      ],
      "metadata": {
        "id": "Rh-LsMGHHzbD"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "\n",
        "genai.configure(api_key=\"GOOGLE_API_KEY\")\n",
        "\n",
        "class GeminiEmbeddingFunction(Embeddings):\n",
        "    def embed_documents(self, texts):\n",
        "        return [\n",
        "            genai.embed_content(\n",
        "                model=\"models/embedding-001\",\n",
        "                content=text,\n",
        "                task_type=\"retrieval_document\"\n",
        "            )[\"embedding\"] for text in texts\n",
        "        ]\n",
        "    def embed_query(self, text):\n",
        "        return genai.embed_content(\n",
        "            model=\"models/embedding-001\",\n",
        "            content=text,\n",
        "            task_type=\"retrieval_query\"\n",
        "        )[\"embedding\"]\n"
      ],
      "metadata": {
        "id": "S84g-jwxERuE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    Document(page_content=\"LangChain helps developers build LLM applications easily.\"),\n",
        "    Document(page_content=\"Chroma is a vector database optimized for LLM-based search.\"),\n",
        "    Document(page_content=\"Embeddings convert text into high-dimensional vectors.\"),\n",
        "    Document(page_content=\"OpenAI provides powerful embedding models.\"),\n",
        "]"
      ],
      "metadata": {
        "id": "Eljc51myFgOJ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Chroma vector store in memory\n",
        "vector_store = Chroma.from_documents(\n",
        "    embedding=GeminiEmbeddingFunction(),\n",
        "    persist_directory='my_chroma_db_new',\n",
        "    documents=documents,\n",
        "    collection_name='sample'\n",
        ")"
      ],
      "metadata": {
        "id": "ABIJUhZkEXLP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert vectorstore into a retriever\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "5zh6TmgAGB0A"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Chroma used for?\"\n",
        "results = retriever.invoke(query)"
      ],
      "metadata": {
        "id": "uTfPm0XTGJlX"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(doc.page_content)\n",
        "# print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukr6OnBqGOzc",
        "outputId": "39d0089b-0470-45e9-da3d-366ae695c4c9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Result 1 ---\n",
            "Chroma is a vector database optimized for LLM-based search.\n",
            "\n",
            "--- Result 2 ---\n",
            "Embeddings convert text into high-dimensional vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search(query, k=2)"
      ],
      "metadata": {
        "id": "uVr8Ubl4GRI9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7Qxp-FXGUYX",
        "outputId": "a9ef1853-13a2-4a8f-a03f-cb9deb0a5f04"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Result 1 ---\n",
            "Chroma is a vector database optimized for LLM-based search.\n",
            "\n",
            "--- Result 2 ---\n",
            "Embeddings convert text into high-dimensional vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UwvhZpeGsG8",
        "outputId": "519b1b59-45f0-4981-f4f3-d0b82c6a3aab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0cwcOoksGtQG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}