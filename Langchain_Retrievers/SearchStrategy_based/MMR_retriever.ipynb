{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain chromadb faiss-cpu openai tiktoken langchain_openai langchain-community wikipedia"
      ],
      "metadata": {
        "id": "TQVEdlAPEQI0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-chroma -q"
      ],
      "metadata": {
        "id": "Rh-LsMGHHzbD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "\n",
        "genai.configure(api_key=\"GOOGLE_API_KEY\")\n",
        "\n",
        "class GeminiEmbeddingFunction(Embeddings):\n",
        "    def embed_documents(self, texts):\n",
        "        return [\n",
        "            genai.embed_content(\n",
        "                model=\"models/embedding-001\",\n",
        "                content=text,\n",
        "                task_type=\"retrieval_document\"\n",
        "            )[\"embedding\"] for text in texts\n",
        "        ]\n",
        "    def embed_query(self, text):\n",
        "        return genai.embed_content(\n",
        "            model=\"models/embedding-001\",\n",
        "            content=text,\n",
        "            task_type=\"retrieval_query\"\n",
        "        )[\"embedding\"]\n"
      ],
      "metadata": {
        "id": "S84g-jwxERuE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    Document(page_content=\"LangChain makes it easy to work with LLMs.\"),\n",
        "    Document(page_content=\"LangChain is used to build LLM based applications.\"),\n",
        "    Document(page_content=\"Chroma is used to store and search document embeddings.\"),\n",
        "    Document(page_content=\"Embeddings are vector representations of text.\"),\n",
        "    Document(page_content=\"MMR helps you get diverse results when doing similarity search.\"),\n",
        "    Document(page_content=\"LangChain supports Chroma, FAISS, Pinecone, and more.\"),\n",
        "]"
      ],
      "metadata": {
        "id": "Eljc51myFgOJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Initialize OpenAI embeddings\n",
        "embedding_model = GeminiEmbeddingFunction()\n",
        "\n",
        "# Step 2: Create the FAISS vector store from documents\n",
        "vectorstore = FAISS.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding_model\n",
        ")"
      ],
      "metadata": {
        "id": "PcjWAHTNjKLL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable MMR in the retriever\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",                   # <-- This enables MMR\n",
        "    search_kwargs={\"k\": 3, \"lambda_mult\": 0.5}  # k = top results, lambda_mult = relevance-diversity balance\n",
        ")"
      ],
      "metadata": {
        "id": "DgofG-Bjjcak"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is langchain?\"\n",
        "results = retriever.invoke(query)"
      ],
      "metadata": {
        "id": "-aqKFyUqjc9x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHvn8lbkje04",
        "outputId": "c10d8d97-e7fc-4e79-dffd-5ede4bdd979a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Result 1 ---\n",
            "LangChain supports Chroma, FAISS, Pinecone, and more.\n",
            "\n",
            "--- Result 2 ---\n",
            "LangChain is used to build LLM based applications.\n",
            "\n",
            "--- Result 3 ---\n",
            "Embeddings are vector representations of text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gL7E3KQojgnO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}